{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 备忘录-PyTorch教程\n",
    "+ 官方教学见: [Torch](https://pytorch.org/docs/stable/torch.html#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似Python但变成张量(Tensor)\n",
    "| Python | PyTorch |\n",
    "| - | - |\n",
    "| int | IntTensor of size() |\n",
    "| float | FloatTensor of size() |\n",
    "| Int array | IntTensor of size[d1, d2, ...] |\n",
    "| Float array | FloatTensor of size[d1, d2, ...] |\n",
    "| string | 无 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 获取数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(2, 3) # 定义一个两行三列的tensor\n",
    "# randn表示创建的数据满足N（0,1）的正态分布\n",
    "\n",
    "data.type() # 显示该变量的数据类型\n",
    "# 其他方式\n",
    "# type(a)\n",
    "# output: torch.Tensor\n",
    "# isinstance(a, torch.FloatTensor) \n",
    "# output: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (与本文内容无关, 清除本节用到的变量, 防止干扰)\n",
    "%xdel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 注意在CPU和GPU中tensor数据类型是不同\n",
    "在CPU中是`torch.FloatTensor`  \n",
    "在GPU中是`torch.cuda.FloatTensor`(多了一个`cuda`)\n",
    "(其他数据类型类似)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意在CPU和GPU中tensor数据类型是不同的\n",
    "# 在CPU中\n",
    "dataInCpu = torch.randn(3, 4)\n",
    "dataInCpu.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在GPU中\n",
    "dataInGpu = dataInCpu.cuda()\n",
    "dataInGpu.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (与本文内容无关, 清除本节用到的变量, 防止干扰)\n",
    "%xdel dataInCpu\n",
    "%xdel dataInGpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 张量(tensor), 维度(dimension), 形状(shape)\n",
    "tensor, dimension(dim)/rank, size/shape的区别\n",
    "#### A. 张量(tensor) \n",
    "tensor, PyTorch中用于表示多维数据的主要数据结构, 类似于多维数组, 可以存储和操作数字数据.  \n",
    "0维称为标量, 一维张量称为向量, 二维张量称为矩阵, 任何超过二维的东西通常都被称为张量."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. 维度(dim/rank)\n",
    "dimension(dim)/rank, 数据科学中记为dimension, 在数学记为rank.   \n",
    "是指该张量的轴数或阶数, 即张量的维度 = 轴数 = 阶数.  \n",
    "+ 如何理解维度\n",
    "    ![维度](./img/维度.png)\n",
    "    如图, 最边边有几层方括号就是几维.   \n",
    "+ 查看当前张量的维度\n",
    "    ```ipython\n",
    "    x = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "    x.dim()\n",
    "    ```\n",
    "    **Output:**\n",
    "    ```\n",
    "    3\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. 形状(shape/size)\n",
    "size/shape, 是张量的形状, 列入3行4列的矩阵, 则size=[3, 4]\n",
    "+ 如何理解形状\n",
    "    例如, 形状为[2, 3]就是2列3行的矩阵  \n",
    "+ 查看当前张量的形状\n",
    "    ```ipython\n",
    "    y = torch.rand(2, 3, 4) # 创建一个形状为[2, 3, 4]的三维张量\n",
    "    ```\n",
    "    + 方法1: `shape`\n",
    "        ```ipython     \n",
    "        y.shape\n",
    "        ```\n",
    "    + 方法2： `size()`\n",
    "        ```ipython\n",
    "        y.size()\n",
    "        ```\n",
    "    输出都是\n",
    "    ```\n",
    "    torch.Size([2, 3, 4])\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 张量创建汇总\n",
    " \n",
    "| 类型 | 方法 | 说明 |\n",
    "| - | - | - |\n",
    "| 直接给数据 | 标量: `data0 = torch.tensor(1.3)` </br>向量: `data1 = torch.tensor([1, 2, 3])` </br>张量: `data2 = torch.tensor([[1], [2], [3]])` | **小**写`tensor`直接将数据赋值给`data` |  \n",
    "| 按形状创建 | 标量: 不可行 </br>向量: `data1 = torch.FloatTensor(4)` </br>张量: `data2 = torch.FloatTensor(2, 3)` | **大**写`FloatTensor`按所给size创建张量`data`. 大写`Tensor`同理.  |  \n",
    "| 未初始化 | `zeros = torch.empty(2, 3)` | |\n",
    "| 全零 | `zeros = torch.zeros(2, 3)` | |\n",
    "| 全一 | `ones = torch.ones(2, 3)` | |\n",
    "| 随机值 | `torch.manual_seed(1729)  # 设置随机种子(可选)`</br>`random = torch.rand(2, 3)` | |\n",
    "| 正态分布 | `data = torch.randn(2, 3)` | 创建的数据满足N(0,1)的正态分布 |\n",
    "| 张量形状模仿 | `x = torch.empty(2, 2, 3) #模板张量`</br>`empty_like_x = torch.empty_like(x) # 根据模板创建一个相同形状的张量` | 对两个或多个张量执行操作时，需要它们具有相同的形状(即具有相同的维度数以及每个维度中相同的单元数). 为此，我们有以下`torch.*_like()`方法(如`zeros_like`, `ones_like`, `rand_like`) |\n",
    "| 设置张量的数据类型 | 一般方法: </br>`a = torch.ones((2, 3), dtype=torch.int16)` | 其他数据类型: </br>`torch.bool`, `torch.int8`, `torch.uint8`, `torch.int16`, `torch.int32`, `torch.int64`, `torch.half`, `torch.float`, `torch.double`, `torch.bfloat` | \n",
    "|  | 强制转换实现方法: </br>`b = torch.rand((2, 3), dtype=torch.float64) * 20.` </br>`print(b)` </br>`c = b.to(torch.int32)` </br>`print(c)` | \n",
    "| Numpy方法 |`data = np.ones(2)` </br>`torch.from_numpy(data)` | 先np后转换, 数值默认为1 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8068e-07, 1.5723e-42, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0 = torch.FloatTensor(4, 4)\n",
    "data0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 标量 (0维数据)\n",
    "Dimension=0 / rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 如何定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个标量/整数int\n",
    "data1 = torch.tensor(1.) # 直接将数据1赋值\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个标量/变量 示例2\n",
    "data2 = torch.tensor(1.3) # 直接将数据1赋值\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (与本文无关, 清除本节用到的变量, 防止干扰)\n",
    "%xdel data1\n",
    "%xdel data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 张量(1维数组)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 如何定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3000, 2.5000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法1: 直接将数据[1.3]或[4.3, 2.5]赋值给data0或data1\t\n",
    "data0 = torch.tensor([1.3]) \n",
    "data1 = torch.tensor([4.3, 2.5])\n",
    "# data0\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7888e-07, 1.5723e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法2: 生成一个长度为5的一维张量data, 数值随机\t\n",
    "data2 = torch.FloatTensor(5)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法3: (通过numpy方法)先np后转换, 数值默认为1\t\n",
    "import numpy as np\n",
    "data3 = np.ones(2)\n",
    "# 转换方法\n",
    "torch.from_numpy(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (与本文无关, 清除本节用到的变量, 防止干扰)\n",
    "%xdel data0\n",
    "%xdel data1\n",
    "%xdel data2\n",
    "%xdel data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. 概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y. 参数/属性\n",
    "参数名称, 意义, 参考值等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.Tensor.requires_grad`\n",
    "1. 作用:   \n",
    "    True需要为此张量计算梯度，否则为False  \n",
    "    如果设置它的属性 `.requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。\n",
    "    如果一个张量的`requires_grad=True`,那么在调用`backward()`方法时反向传播计算梯度,我们会为这个张量计算梯度, 但是计算完梯度之后这个梯度并不一定会一直保存在属性`grad`中.只有对于`requires_grad=True`的叶子张量,我们才会将梯度一直保存在该叶子张量的`grad`属性中,对于非叶子节点,即中间节点的张量,我们在计算完梯度之后为了更高效地利用内存,我们会将梯度grad的内存释放掉.)\n",
    "    \n",
    "2. 参考值:\n",
    "    + `requires_grad=True`时，表示参数需要参与训练，并在训练过程中进行梯度计算\n",
    "    + `requires_grad=False`时，表示参数不需要参与训练，可以将代码块包装在 with torch.no_grad(): 中\n",
    "\n",
    "3. 示例\n",
    "    ```python\n",
    "    import torch\n",
    "    w = torch.tensor([2.5], requires_grad=True)\n",
    "    # 或\n",
    "    x = torch.arange(4.0)\n",
    "    x.requires_grad_(True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.Tensor.grad`\n",
    "1. 作用:    \n",
    "    gradient, 梯度.  \n",
    "    查看梯度的计算结果.\n",
    "2. 注意:\n",
    "    需要该张量已经设置属性`requires_grad=True`, 且已经计算了梯度`backward`\n",
    "\n",
    "3. 示例  \n",
    "    见`备忘录-数学基础.md`梯度示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Z. Torch 常用方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### [模板] 方法名(常用参数) - Ret: 返回值类型\n",
    ">`方法名(全部参数)` (去[官网](https://pytorch.org/docs/stable/torch.html)拷贝)\n",
    "+ **Return** 方法作用\n",
    "+ **Parameters** \n",
    "    | 参数(必选*) | 类型 | 说明 | 默认值 | 建议 | \n",
    "    | - | - | - | - | - | \n",
    "    | 参数名* | 数值类型 |  |  | \n",
    "+ **Example**\n",
    "    ```ipython notebook\n",
    "    >>> torch.arange(5)\n",
    "    tensor([ 0,  1,  2,  3,  4])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 求和 - `torch.sum(input, dim, keepdim=False, *, dtype=None)` - Ret: `Tensor`\n",
    "> `torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor`\n",
    "\n",
    "+ **Return** 根据`dim`维度, 返回`input`张量的每行的总和. \n",
    "    如果`dim`是维度列表, 则减少所有维度.\n",
    "    \n",
    "+ **Parameters** \n",
    "    | 参数(必选*) | 类型 | 说明 | 默认值 | 建议 | \n",
    "    | - | - | - | - | - | \n",
    "    | input* | tensor | 被求和的张量 |  |  |  \n",
    "    | dim | int or tuple of ints | 要减少的一个或多个维度 |  |  |  \n",
    "    | keepdim | bool | 输出张量是否dim保留 | `None` |  |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建张量, \n",
    "a = torch.arange(2 * 3 * 4).view(2, 3, 4)\n",
    "a   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " tensor([[[12, 15, 18, 21]],\n",
       " \n",
       "         [[48, 51, 54, 57]]]),\n",
       " tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例1: 求每列的和\n",
    "torch.sum(a, 1), torch.sum(a, 1, keepdim=True), torch.sum(a, 1, keepdim=False), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 66, 210]),\n",
       " tensor([[[ 66]],\n",
       " \n",
       "         [[210]]]),\n",
       " tensor([ 66, 210]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, (2, 1)), torch.sum(a, (2, 1), keepdim=True), torch.sum(a, (2, 1), keepdim=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建等差张量 - `torch.arange(start=0, end, step=1)` - Ret: `tensor`\n",
    ">`torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "+ **Return** 返回一维张量区间为[start, end], 步长为step的等差张量\n",
    "+ **Parameters** \n",
    "    | 参数(必选*) | 类型 | 说明 | 默认值 | 建议 | \n",
    "    | - | - | - | - | - | \n",
    "    | start* | Number | 起始值 | 0 |  \n",
    "    | end* | Number | 结束值 | | \n",
    "    | step* | Number |  步长 | 1 |\n",
    "    | out | Tensor | 输出张量 | | \n",
    "    | dtype | torch.dtype | 返回张量所需的数据类型 | 如果None，则使用全局默认值（请参阅torch.set_default_dtype()）。如果未给出dtype，则从其他输入参数推断数据类型。如果start、end或stop中的任何一个是浮点型， 则数据类型将被推断为默认数据类型，请参阅 get_default_dtype()。否则，dtype被推断为torch.int64。|\n",
    "    | layout | torch.layout | 返回张量的所需布局 | torch.strided | \n",
    "    | device  | torch.device | 返回张量所需的设备 | 如果None，则使用当前设备作为默认张量类型（请参阅torch.set_default_device()）。device对于 CPU 张量类型，将是 CPU；对于 CUDA 张量类型，将是当前 CUDA 设备 | \n",
    "    | require_grad | bool | autograd 是否应记录对返回张量的操作 | False |\n",
    "+ **Example**\n",
    "    ```ipython notebook\n",
    "    >>> torch.arange(5)\n",
    "    tensor([ 0,  1,  2,  3,  4])\n",
    "    >>> torch.arange(1, 4)\n",
    "    tensor([ 1,  2,  3])\n",
    "    >>> torch.arange(1, 2.5, 0.5)\n",
    "    tensor([ 1.0000,  1.5000,  2.0000])\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)\n",
    "torch.arange(1, 4)\n",
    "torch.arange(1, 2.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 记录梯度值 - torch.Tensor.requires_grad_ - Ret: tensor\n",
    ">`Tensor.requires_grad_(requires_grad=True) → Tensor` \n",
    "+ **Return** 设置梯度记录为开启, 详情见属性`requires_grad`\n",
    "+ **Parameters** \n",
    "    | 参数(必选*) | 类型 | 说明 | 默认值 | 建议 | \n",
    "    | - | - | - | - | - | \n",
    "    | requires_grad* | bool | 是否记录计算的梯度值 | True | True\\|False |  \n",
    "+ **Example**\n",
    "    ```ipython notebook\n",
    "    data = torch.arange(4.0)\n",
    "    data.requires_grad_(True)  # 等价于data=torch.arange(4.0,requires_grad=True)\n",
    "    data.grad  # 默认值是None\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(4.0)\n",
    "data.requires_grad_(True)  # 等价于data=torch.arange(4.0,requires_grad=True)\n",
    "data.grad  # 默认值是None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量内存大小 - `.numel()`  \n",
    "就是该张量size的乘积  \n",
    "如下示例, `.numel()`返回24 = 2 * 3 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(2, 3, 4) # 准备工作 - 创建一个张量\n",
    "data.numel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.flatten(input, start_dim=0, end_dim=-1)` - Ret: Tensor\n",
    ">`torch.flatten(input, start_dim=0, end_dim=-1)`\n",
    "+ **Return** 展平张量/给多维张量降维\n",
    "+ **Parameters** \n",
    "    | 参数(必选*) | 类型 | 说明 | 默认值 | 建议 | \n",
    "    | - | - | - | - | - | \n",
    "    | input* | tensor | 输入的张量 |  | \n",
    "    | start_dim | int |  |  | \n",
    "    | end_dim | int |  |  | \n",
    "+ **Example**\n",
    "    ```ipython notebook\n",
    "    >>> torch.arange(5)\n",
    "    tensor([ 0,  1,  2,  3,  4])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " tensor([[1, 2, 3, 4],\n",
       "         [5, 6, 7, 8]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    [[[1, 2],[3, 4]],\n",
    "     [[5, 6],[7, 8]]])\n",
    "torch.flatten(t), torch.flatten(t, start_dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
